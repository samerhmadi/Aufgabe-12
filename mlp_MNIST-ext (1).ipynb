{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN Intro extended\n",
    "\n",
    "This NB is constructed such that it also runs in reasonable time on Laptop CPUs (e.g. an i3)\n",
    "\n",
    "It builds on the NB from the last practical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Setup\n",
    "\n",
    "There are different options to set up the TensorFlow library (which now includes [Keras](https://keras.io) as backend library) on your own computer. The simplest of them is using only the CPU and can be installed in 1 command via [`conda`](https://docs.anaconda.com/anaconda/user-guide/tasks/tensorflow/), in an anaconda shell run:\n",
    "\n",
    "```\n",
    "conda install tensorflow\n",
    "```\n",
    "\n",
    "**NOTE**: TF migth not be compatible with your current environment, so here we create a [new environment](https://conda.io/docs/user-guide/tasks/manage-environments.html#creating-an-environment-with-commands) first:\n",
    "\n",
    "```\n",
    "conda create -n tf tensorflow\n",
    "conda activate tf\n",
    "```\n",
    "\n",
    "In that case you need to install jupyter, scikit-learn, matplotlib, numpy and pandas in that environment again, with e.g.:\n",
    "\n",
    "```\n",
    "conda install jupyter scikit-learn matplotlib numpy pandas\n",
    "```\n",
    "\n",
    "(If you have a [supported Nvidia graphics card](https://developer.nvidia.com/cuda-gpus) in your machine and would like to use it for accelerated network training, make sure to follow [this guide](https://www.tensorflow.org/install/gpu) to install required packages and finally use the `tensorflow-gpu` library.)\n",
    "\n",
    "The usage of the TensorFlow library in Python will be the same for CPU and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## fetch MNIST dataset (as done in last NB)\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel1      0.000000\n",
      "pixel2      0.000000\n",
      "pixel3      0.000000\n",
      "pixel4      0.000000\n",
      "pixel5      0.000000\n",
      "              ...   \n",
      "pixel780    0.243137\n",
      "pixel781    0.000000\n",
      "pixel782    0.000000\n",
      "pixel783    0.000000\n",
      "pixel784    0.000000\n",
      "Length: 784, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50000, 784)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Scale the input data into the range [0, 1]\n",
    "## use sklearn's train_test_split to split the data into \n",
    "## 50000 instances for training, 10000 for validation and 10000 for testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X= mnist.data\n",
    "X= X / 255.0\n",
    "print(X.max())\n",
    "y= mnist.target\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=10000, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=10000, random_state=42)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to test your TensorFlow installation by importing the package. The following code cell should execute without errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check which computing devices TensorFlow has found on this machine. If you don't have the GPU setup on your computer, the list should just contain one CPU: `/device:CPU:0` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9729867130500926123\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below creates a similar MLP as above using tf.keras, see also this [tutorial network](https://github.com/keras-team/keras/blob/fcc0bfa354c5a47625d681d0297a66ef9ff43a9e/examples/mnist_mlp.py) which also uses the MNIST dataset.\n",
    "\n",
    "Keras has a nice method `model.summary()` that prints a tabular overview of your network architecture, together with the input/output dimensions and number of parameters for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "2.4.0\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 84,060\n",
      "Trainable params: 84,060\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.3567 - accuracy: 0.8983 - val_loss: 0.1842 - val_accuracy: 0.9474\n",
      "Epoch 2/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.1495 - accuracy: 0.9564 - val_loss: 0.1367 - val_accuracy: 0.9607\n",
      "Epoch 3/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.1066 - accuracy: 0.9677 - val_loss: 0.1048 - val_accuracy: 0.9675\n",
      "Epoch 4/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0814 - accuracy: 0.9753 - val_loss: 0.1044 - val_accuracy: 0.9692\n",
      "Epoch 5/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0658 - accuracy: 0.9803 - val_loss: 0.0919 - val_accuracy: 0.9722\n",
      "Epoch 6/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0539 - accuracy: 0.9830 - val_loss: 0.0924 - val_accuracy: 0.9715\n",
      "Epoch 7/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0435 - accuracy: 0.9866 - val_loss: 0.0842 - val_accuracy: 0.9735\n",
      "Epoch 8/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0368 - accuracy: 0.9885 - val_loss: 0.0828 - val_accuracy: 0.9753\n",
      "Epoch 9/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0313 - accuracy: 0.9901 - val_loss: 0.0908 - val_accuracy: 0.9737\n",
      "Epoch 10/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0254 - accuracy: 0.9921 - val_loss: 0.0852 - val_accuracy: 0.9744\n",
      "Epoch 11/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0229 - accuracy: 0.9927 - val_loss: 0.0993 - val_accuracy: 0.9733\n",
      "Epoch 12/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0184 - accuracy: 0.9943 - val_loss: 0.0977 - val_accuracy: 0.9743\n",
      "Epoch 13/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0158 - accuracy: 0.9953 - val_loss: 0.0943 - val_accuracy: 0.9754\n",
      "Epoch 14/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0147 - accuracy: 0.9955 - val_loss: 0.1001 - val_accuracy: 0.9744\n",
      "Epoch 15/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0145 - accuracy: 0.9954 - val_loss: 0.0996 - val_accuracy: 0.9756\n",
      "Epoch 16/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0117 - accuracy: 0.9962 - val_loss: 0.1005 - val_accuracy: 0.9763\n",
      "Epoch 17/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0105 - accuracy: 0.9967 - val_loss: 0.1138 - val_accuracy: 0.9745\n",
      "Epoch 18/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0115 - accuracy: 0.9962 - val_loss: 0.1338 - val_accuracy: 0.9705\n",
      "Epoch 19/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0090 - accuracy: 0.9974 - val_loss: 0.1160 - val_accuracy: 0.9733\n",
      "Epoch 20/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0072 - accuracy: 0.9977 - val_loss: 0.1134 - val_accuracy: 0.9760\n",
      "Epoch 21/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0087 - accuracy: 0.9971 - val_loss: 0.1282 - val_accuracy: 0.9744\n",
      "Epoch 22/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0084 - accuracy: 0.9972 - val_loss: 0.1286 - val_accuracy: 0.9747\n",
      "Epoch 23/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0047 - accuracy: 0.9988 - val_loss: 0.1277 - val_accuracy: 0.9743\n",
      "Epoch 24/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0093 - accuracy: 0.9970 - val_loss: 0.1345 - val_accuracy: 0.9745\n",
      "Epoch 25/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0065 - accuracy: 0.9979 - val_loss: 0.1436 - val_accuracy: 0.9734\n",
      "Epoch 26/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0069 - accuracy: 0.9979 - val_loss: 0.1294 - val_accuracy: 0.9756\n",
      "Epoch 27/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0039 - accuracy: 0.9988 - val_loss: 0.1255 - val_accuracy: 0.9762\n",
      "Epoch 28/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0067 - accuracy: 0.9978 - val_loss: 0.1296 - val_accuracy: 0.9762\n",
      "Epoch 29/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0113 - accuracy: 0.9963 - val_loss: 0.1442 - val_accuracy: 0.9742\n",
      "Epoch 30/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0054 - accuracy: 0.9982 - val_loss: 0.1352 - val_accuracy: 0.9757\n",
      "Epoch 31/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.1448 - val_accuracy: 0.9738\n",
      "Epoch 32/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0066 - accuracy: 0.9976 - val_loss: 0.1231 - val_accuracy: 0.9769\n",
      "Epoch 33/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.1368 - val_accuracy: 0.9763\n",
      "Epoch 34/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 7.6311e-04 - accuracy: 0.9999 - val_loss: 0.1262 - val_accuracy: 0.9790\n",
      "Epoch 35/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.8144e-04 - accuracy: 1.0000 - val_loss: 0.1258 - val_accuracy: 0.9796\n",
      "Epoch 36/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.0651e-04 - accuracy: 1.0000 - val_loss: 0.1257 - val_accuracy: 0.9796\n",
      "Epoch 37/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 8.6237e-05 - accuracy: 1.0000 - val_loss: 0.1271 - val_accuracy: 0.9799\n",
      "Epoch 38/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 7.1546e-05 - accuracy: 1.0000 - val_loss: 0.1287 - val_accuracy: 0.9800\n",
      "Epoch 39/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 6.4355e-05 - accuracy: 1.0000 - val_loss: 0.1293 - val_accuracy: 0.9799\n",
      "Epoch 40/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 5.5089e-05 - accuracy: 1.0000 - val_loss: 0.1323 - val_accuracy: 0.9799\n",
      "Epoch 41/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0239 - accuracy: 0.9938 - val_loss: 0.1617 - val_accuracy: 0.9720\n",
      "Epoch 42/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0110 - accuracy: 0.9960 - val_loss: 0.1434 - val_accuracy: 0.9751\n",
      "Epoch 43/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0036 - accuracy: 0.9988 - val_loss: 0.1387 - val_accuracy: 0.9791\n",
      "Epoch 44/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 7.3309e-04 - accuracy: 0.9999 - val_loss: 0.1346 - val_accuracy: 0.9773\n",
      "Epoch 45/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0037 - accuracy: 0.9989 - val_loss: 0.1465 - val_accuracy: 0.9771\n",
      "Epoch 46/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0108 - accuracy: 0.9964 - val_loss: 0.1663 - val_accuracy: 0.9744\n",
      "Epoch 47/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0048 - accuracy: 0.9985 - val_loss: 0.1438 - val_accuracy: 0.9775\n",
      "Epoch 48/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 7.3068e-04 - accuracy: 0.9999 - val_loss: 0.1533 - val_accuracy: 0.9769\n",
      "Epoch 49/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9990 - val_loss: 0.1727 - val_accuracy: 0.9734\n",
      "Epoch 50/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0100 - accuracy: 0.9968 - val_loss: 0.1568 - val_accuracy: 0.9757\n",
      "Epoch 51/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9991 - val_loss: 0.1514 - val_accuracy: 0.9776\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0062 - accuracy: 0.9983 - val_loss: 0.1477 - val_accuracy: 0.9771\n",
      "Epoch 53/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0037 - accuracy: 0.9987 - val_loss: 0.1521 - val_accuracy: 0.9773\n",
      "Epoch 54/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.1420 - val_accuracy: 0.9781\n",
      "Epoch 55/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 2.3230e-04 - accuracy: 1.0000 - val_loss: 0.1494 - val_accuracy: 0.9771\n",
      "Epoch 56/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 8.6424e-05 - accuracy: 1.0000 - val_loss: 0.1418 - val_accuracy: 0.9794\n",
      "Epoch 57/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 4.3176e-05 - accuracy: 1.0000 - val_loss: 0.1423 - val_accuracy: 0.9793\n",
      "Epoch 58/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 3.4793e-05 - accuracy: 1.0000 - val_loss: 0.1436 - val_accuracy: 0.9792\n",
      "Epoch 59/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 2.8924e-05 - accuracy: 1.0000 - val_loss: 0.1452 - val_accuracy: 0.9795\n",
      "Epoch 60/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 2.4324e-05 - accuracy: 1.0000 - val_loss: 0.1461 - val_accuracy: 0.9797\n",
      "Epoch 61/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 2.0788e-05 - accuracy: 1.0000 - val_loss: 0.1473 - val_accuracy: 0.9796\n",
      "Epoch 62/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.7868e-05 - accuracy: 1.0000 - val_loss: 0.1497 - val_accuracy: 0.9794\n",
      "Epoch 63/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.4995e-05 - accuracy: 1.0000 - val_loss: 0.1511 - val_accuracy: 0.9802\n",
      "Epoch 64/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.2644e-05 - accuracy: 1.0000 - val_loss: 0.1526 - val_accuracy: 0.9801\n",
      "Epoch 65/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.0915e-05 - accuracy: 1.0000 - val_loss: 0.1543 - val_accuracy: 0.9800\n",
      "Epoch 66/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 9.3026e-06 - accuracy: 1.0000 - val_loss: 0.1562 - val_accuracy: 0.9794\n",
      "Epoch 67/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 7.8666e-06 - accuracy: 1.0000 - val_loss: 0.1580 - val_accuracy: 0.9794\n",
      "Epoch 68/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 6.7043e-06 - accuracy: 1.0000 - val_loss: 0.1609 - val_accuracy: 0.9796\n",
      "Epoch 69/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 5.6065e-06 - accuracy: 1.0000 - val_loss: 0.1623 - val_accuracy: 0.9798\n",
      "Epoch 70/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 4.8966e-06 - accuracy: 1.0000 - val_loss: 0.1648 - val_accuracy: 0.9797\n",
      "Epoch 71/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 4.1032e-06 - accuracy: 1.0000 - val_loss: 0.1628 - val_accuracy: 0.9802\n",
      "Epoch 72/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0079 - accuracy: 0.9985 - val_loss: 0.3538 - val_accuracy: 0.9564\n",
      "Epoch 73/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0199 - accuracy: 0.9943 - val_loss: 0.1681 - val_accuracy: 0.9771\n",
      "Epoch 74/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9994 - val_loss: 0.1553 - val_accuracy: 0.9789\n",
      "Epoch 75/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.2827e-04 - accuracy: 1.0000 - val_loss: 0.1506 - val_accuracy: 0.9789\n",
      "Epoch 76/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 4.6716e-05 - accuracy: 1.0000 - val_loss: 0.1517 - val_accuracy: 0.9789\n",
      "Epoch 77/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 3.5546e-05 - accuracy: 1.0000 - val_loss: 0.1521 - val_accuracy: 0.9792\n",
      "Epoch 78/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 2.9406e-05 - accuracy: 1.0000 - val_loss: 0.1527 - val_accuracy: 0.9791\n",
      "Epoch 79/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 2.4896e-05 - accuracy: 1.0000 - val_loss: 0.1535 - val_accuracy: 0.9789\n",
      "Epoch 80/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 2.1140e-05 - accuracy: 1.0000 - val_loss: 0.1542 - val_accuracy: 0.9790\n",
      "Epoch 81/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.7756e-05 - accuracy: 1.0000 - val_loss: 0.1552 - val_accuracy: 0.9791\n",
      "Epoch 82/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.5224e-05 - accuracy: 1.0000 - val_loss: 0.1561 - val_accuracy: 0.9793\n",
      "Epoch 83/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.2897e-05 - accuracy: 1.0000 - val_loss: 0.1568 - val_accuracy: 0.9789\n",
      "Epoch 84/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.0905e-05 - accuracy: 1.0000 - val_loss: 0.1584 - val_accuracy: 0.9792\n",
      "Epoch 85/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 9.2375e-06 - accuracy: 1.0000 - val_loss: 0.1590 - val_accuracy: 0.9792\n",
      "Epoch 86/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 7.7331e-06 - accuracy: 1.0000 - val_loss: 0.1607 - val_accuracy: 0.9790\n",
      "Epoch 87/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 6.4980e-06 - accuracy: 1.0000 - val_loss: 0.1617 - val_accuracy: 0.9792\n",
      "Epoch 88/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 5.4768e-06 - accuracy: 1.0000 - val_loss: 0.1625 - val_accuracy: 0.9794\n",
      "Epoch 89/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 4.6222e-06 - accuracy: 1.0000 - val_loss: 0.1652 - val_accuracy: 0.9795\n",
      "Epoch 90/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 3.7554e-06 - accuracy: 1.0000 - val_loss: 0.1656 - val_accuracy: 0.9790\n",
      "Epoch 91/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 3.1399e-06 - accuracy: 1.0000 - val_loss: 0.1670 - val_accuracy: 0.9790\n",
      "Epoch 92/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 2.6194e-06 - accuracy: 1.0000 - val_loss: 0.1701 - val_accuracy: 0.9793\n",
      "Epoch 93/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 2.1836e-06 - accuracy: 1.0000 - val_loss: 0.1723 - val_accuracy: 0.9794\n",
      "Epoch 94/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0207 - accuracy: 0.9955 - val_loss: 0.2131 - val_accuracy: 0.9741\n",
      "Epoch 95/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0107 - accuracy: 0.9967 - val_loss: 0.1778 - val_accuracy: 0.9767\n",
      "Epoch 96/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0038 - accuracy: 0.9988 - val_loss: 0.1837 - val_accuracy: 0.9781\n",
      "Epoch 97/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 6.0240e-04 - accuracy: 0.9998 - val_loss: 0.1647 - val_accuracy: 0.9797\n",
      "Epoch 98/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0066 - accuracy: 0.9981 - val_loss: 0.2256 - val_accuracy: 0.9738\n",
      "Epoch 99/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0062 - accuracy: 0.9981 - val_loss: 0.1717 - val_accuracy: 0.9788\n",
      "Epoch 100/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 0.1625 - val_accuracy: 0.9781\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "batch_size = 100\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "## convert class vectors to binary class matrices\n",
    "y_train_c = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val_c = keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test_c = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape=(784,)))\n",
    "model.add(Dense( 50, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train_c,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val, y_val_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.19798025488853455\n",
      "Test accuracy: 0.9769999980926514\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test_c, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the training going through the epochs and in the end the trained network is evaluated on the test set. \n",
    "It shoud reach at least a classification accurary of 97% on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "If you are interested, there is a special [intro to Keras for engineers](https://keras.io/getting_started/intro_to_keras_for_engineers/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Create a visualization of the model architecture (graph) of the keras model composed above using [`plot_model`](https://keras.io/api/utils/model_plotting_utils/). You will probably need to install `pydot`:\n",
    "\n",
    "`conda install pydot`\n",
    "\n",
    "As you can see [here](https://keras.io/guides/training_with_built_in_methods/#passing-data-to-multiinput-multioutput-models), ANN architectures can be more complex, like e.g. [GoogLeNet](https://miro.medium.com/max/2700/1*ZFPOSAted10TPd3hBQU8iQ.png)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print again the model summary (for easiy comparison with the following visualization)\n",
    "\n",
    "## create a visualization of the model architecture (graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The methode [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) from Keras yields a `history` objekt, which can be used to plot the loss and acc determined during training. This can be done quite selectively (see e.g. [here](https://www.kaggle.com/danbrice/keras-plot-history-full-report-and-grid-search)) or with just a few lines using [`pandas`](https://nbviewer.jupyter.org/github/ageron/handson-ml2/blob/92b5f83f8a/10_neural_nets_with_keras.ipynb#Building-an-Image-Classifier).\n",
    "\n",
    "It is also possible to use [TensorBoard](https://www.tensorflow.org/tensorboard) for visualization but this can be more challanging to [get running](https://nbviewer.jupyter.org/github/ageron/handson-ml2/blob/92b5f83f8a/10_neural_nets_with_keras.ipynb#TensorBoard).\n",
    "\n",
    "### Exercise 2\n",
    "\n",
    "Plot the loss and accuracy of the train and the val set and save the plot as SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## plot loss and acc or the train and val set (hint: can be done with just 5 lines of code)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "1. What could be the reason for peaks in the loss curves?\n",
    "2. How could those peaks be avoided?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers\n",
    "\n",
    "1. \n",
    "2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional 1\n",
    "\n",
    "If you have an idea how to avoid/reduce such peaks, try it out and plot the resulting curves again!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Now plot the loss and accuracy of the train and the test set (and save the plot as SVG) of your optimized model (from the last exercise of the last notebook), that should reach at least 98% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## put the construction and training of your best model here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sorce your best model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot loss and acc or the train and val set of your best model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Compare the two plots. \n",
    "\n",
    "1. What difference do you see? \n",
    "2. Can you explain them?\n",
    "\n",
    "### Answers\n",
    "\n",
    "1. \n",
    "2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the model for \"production\" on a special device (e.g. cash mashine, mobile phone) or in a web service, the model needs to be exported and loaded on the target device.\n",
    "\n",
    "### Exercise 5\n",
    "\n",
    "Export your best model in H5 format and then load it again (as you would do on a target device) wiht a different name and test it by running predictions (scoring) on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## export model as H5\n",
    "\n",
    "## load model from H5 under different name\n",
    "\n",
    "## sore the test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model saving can also be used for `EarlyStopping`.\n",
    "\n",
    "### Optional 2\n",
    "\n",
    "Train your best model until it obviously overfits the validation data. Use an EarlyStopping callback to get the model when the validation loss/acc was best, following this [tutorial](https://nbviewer.jupyter.org/github/ageron/handson-ml2/blob/92b5f83f8a/10_neural_nets_with_keras.ipynb#10.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally check how your best model performs on the (never seen) test set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional 3\n",
    "\n",
    "To get a feeling how much work it is to create a training dataset, test the model prediction on some digits you wrote yourself!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
